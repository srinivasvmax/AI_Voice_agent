version: '3.8'

services:
  llama-server:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: llama-server
    ports:
      - "8080:8080"
    volumes:
      - ../models:/models
    command: >
      --model /models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
      --ctx-size 2048
      --port 8080
      --host 0.0.0.0
    restart: unless-stopped

  voice-agent:
    build: 
      context: ..
      dockerfile: docker/Dockerfile
    container_name: voice-agent
    ports:
      - "8000:8000"
    volumes:
      - ../models:/app/models
      - ../services:/app/services
      - ../web_app.py:/app/web_app.py
      - ../voice_agent.py:/app/voice_agent.py
    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - llama-server
    restart: unless-stopped
